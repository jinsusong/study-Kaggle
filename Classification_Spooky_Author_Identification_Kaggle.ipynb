{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification_Spooky Author Identification_Kaggle.ipynb",
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPv6AV54PZMLtQLStcNiuc3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinsusong/study-Kaggle/blob/main/Classification_Spooky_Author_Identification_Kaggle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kaggle Spooky Author Identification 데이터 불러오기 "
      ],
      "metadata": {
        "id": "fH3HavkYSLPT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2-tBr0sPiLh"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle\n",
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade --force-reinstall --no-deps kaggle"
      ],
      "metadata": {
        "id": "o_TsaUm9RK2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# Permission Warning 방지\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!pwd"
      ],
      "metadata": {
        "id": "HxKQxS0wPkNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c spooky-author-identification"
      ],
      "metadata": {
        "id": "MPTuOJAoPsTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -al"
      ],
      "metadata": {
        "id": "Smx1GhkAQI_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 압축 해제"
      ],
      "metadata": {
        "id": "AcXDnZ8ERxrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "!unzip sample_submission.zip -d data/\n",
        "!unzip test.zip -d data/\n",
        "!unzip train.zip -d data/"
      ],
      "metadata": {
        "id": "9S31m9IcSQON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 확인하기 "
      ],
      "metadata": {
        "id": "IRAY-sgqSXyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "\n",
        "label_cols = ['author']\n",
        "train = pd.read_csv(\"data/train.csv\")\n",
        "print(len(train))\n",
        "#len_eap = train['author'] == 'EAP'\n",
        "#len_eap\n",
        "train.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "8thF4OoPTP0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test = pd.read_csv(\"data/test.csv\")\n",
        "test.head()\n"
      ],
      "metadata": {
        "id": "cp2y829NTQnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "HDsb-ejYuZD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 버트 토크나이저 로드 "
      ],
      "metadata": {
        "id": "d4D0X3kGTlmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "bert_model_name = 'bert-base-uncased'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n"
      ],
      "metadata": {
        "id": "DdXSQeODUbDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenized"
      ],
      "metadata": {
        "id": "hNBrXu-ax4fR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_encodings = tokenizer(X_train, padding=True, truncation=True, max_length=512,add_special_tokens=True, return_attention_mask=True) \n",
        "# val_encodings = tokenizer(X_val, padding=True, truncation=True, max_length=512)\n",
        "# len(train_encodings)\n",
        "from tqdm import tqdm\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "MAX_LEN = 128\n",
        "def tokenize_sentences(sentences, tokenizer, max_seq_len = 128):\n",
        "    tokenized_sentences = []\n",
        "\n",
        "    for sentence in tqdm(sentences):\n",
        "        tokenized_sentence = tokenizer.encode(\n",
        "            sentence, # Sentence to encode\n",
        "            add_special_tokens  = True , # Add '[CLS]' and '[SEP]'\n",
        "            max_length = max_seq_len, # Truencate all sentences,\n",
        "        )\n",
        "        tokenized_sentences.append(tokenized_sentence)\n",
        "    \n",
        "    return tokenized_sentences\n",
        "\n",
        "def create_attention_masks(tokenized_and_padded_sentences):\n",
        "    attention_masks = []\n",
        "\n",
        "    for sentence in tokenized_and_padded_sentences:\n",
        "        att_mask = [int(token_id > 0) for token_id in sentence]\n",
        "        attention_masks.append(att_mask)\n",
        "        \n",
        "    return np.asarray(attention_masks)\n",
        "\n",
        "# 토크나이징 작업\n",
        "input_ids = tokenize_sentences(train['text'], tokenizer, MAX_LEN)\n",
        "print(\"토크나이저 후 \" , input_ids)\n",
        "\n",
        "# 패딩 작업 \n",
        "# maxlen : 정수, 모든 시퀀스의 최대 길이\n",
        "# dtype : 출력 시퀀스의 자료형. \n",
        "# padding: 'pre' 혹은 'post': 각 시퀀스의 처음 혹은 끝을 패딩\n",
        "# truncating: 'pre' 혹은 'post': maxlen보다 큰 시퀀스의 처음 혹은 끝의 값들을 제거\n",
        "# value: 부동소수점 혹은 문자열, 패딩할 값.\n",
        "\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
        "print(\"패딩 후 : \", input_ids)\n",
        "\n",
        "# 패딩 된 데이터를 기반으로 어텐션 마스크 생성\n",
        "attention_masks = create_attention_masks(input_ids)\n",
        "print(\"어텐션 마스크 생성 :\", attention_masks)"
      ],
      "metadata": {
        "id": "qWCNIZy_yJh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocess Data "
      ],
      "metadata": {
        "id": "w9TY8aFDVFZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "# X = list(train[\"text\"])\n",
        "# y = list(train[\"author\"])\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)"
      ],
      "metadata": {
        "id": "cXVpkJH1xYPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train\n",
        "#y_train"
      ],
      "metadata": {
        "id": "cJWwN6ToQS9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(X_train_tokenized)\n",
        "#print(X_train_tokenized)"
      ],
      "metadata": {
        "id": "Wfn15pYVES-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['author']\n",
        "labels =  train['author'].values\n",
        "print(labels)\n",
        "\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=0, test_size=0.1)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=0, test_size=0.1)\n",
        "\n",
        "train_size = len(train_inputs)\n",
        "validation_size = len(validation_inputs)\n"
      ],
      "metadata": {
        "id": "muVAleVTN33W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#데이터 셋으로 변환"
      ],
      "metadata": {
        "id": "T2Q_8LBhEc08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# train_data = tf.data.Dataset.from_tensor_slices(train_encodings)\n",
        "\n",
        "BATCH_SIZE = 32 \n",
        "NR_EPOCHS  = 1\n",
        "\n",
        "def create_dataset(data_tuple, epochs=1, batch_size=32, buffer_size=10000, train=True):\n",
        "    # 데이터 배열이 메모리에 존재한다면, Dataset을 만드는 가장 간단한 방법은 Dataset.from_tensor_slices()를 사용하여 tf.Tensor로 변환하는 것\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(data_tuple) # from_tensor_slices : 차원을 맞춰서 데이터를 변환 ex) .slices(([1,2,],[3,4,],[5,6])) -> ((1,3,5),(2,4,6))\n",
        "    print(dataset)\n",
        "    return dataset \n",
        "\n",
        "\n",
        "train_dataset = create_dataset((train_inputs, train_masks, train_labels), epochs=NR_EPOCHS, batch_size=BATCH_SIZE)\n",
        "validation_dataset = create_dataset((validation_inputs, validation_masks, validation_labels), epochs=NR_EPOCHS, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "b6x7R27FL6yL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_data"
      ],
      "metadata": {
        "id": "4UzFIkvQNypD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(train_data)\n",
        "# i == 0\n",
        "# for element in train_data:\n",
        "#   i += 1\n",
        "#   print(element)\n",
        "#   if i > 1 : break\n",
        "outputs = model(input_ids[:1])\n",
        "outputs"
      ],
      "metadata": {
        "id": "PYjm-wrCQ9ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-phsGcMzL6Ej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model "
      ],
      "metadata": {
        "id": "l7UDYCYPU_OB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFBertForSequenceClassification\n",
        "\n",
        "model = TFBertForSequenceClassification.from_pretrained(bert_model_name)"
      ],
      "metadata": {
        "id": "q463bka5VDzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "# model.compile(optimizer='adam',loss=loss)"
      ],
      "metadata": {
        "id": "HrTWzmGWQbGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.fit(train_dataset, batch_size=32, epochs=1)\n",
        "# #model.fit(train_dataset, validation_data=validation_dataset, batch_size=32, epochs=1)"
      ],
      "metadata": {
        "id": "PcDEZjBRYZip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time \n",
        "from transformers import create_optimizer\n",
        "\n",
        "#  steps_per_epoch : 전체데이터(train_size) // 한번 학습에 사용되는 데이터 수 (BATCH_SIZE)\n",
        "\n",
        "steps_per_epoch = train_size // BATCH_SIZE\n",
        "validation_steps = validation_size // BATCH_SIZE\n",
        "\n",
        "# | Loss Function : 예측 값과 실제 값의 차이를 수치로 표현, 손실함수를 최소값으로 만들기 위해 가중치를 업데이트 함 \n",
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss') # metrics.Mean : 주어진 값의 (가중된) 평균을 계산, ex) [1, 3, 5, 7]이면 평균은 4. 가중치가 [1, 1, 0, 0]으로 지정된 경우 평균은 2\n",
        "validation_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "# tf.keras.metrics.Mean : 훈련 중에 메트릭이 평가될 때 에포크/단계 사이에 호출\n",
        "\n",
        "# | Optimizer (with 1-cycle-policy) : 최적의 학습률을 찾는것 \n",
        "warmup_steps = steps_per_epoch // 3 \n",
        "total_steps = steps_per_epoch * NR_EPOCHS - warmup_steps \n",
        "optimizer = create_optimizer(init_lr=2e-5, num_train_steps=total_steps, num_warmup_steps=warmup_steps)\n",
        "\n",
        "# | Metrics\n",
        "# ROC-AUC Score : 결국 분류의 성능 지표로 사용되는 것, AUC(Area Under Curve) 값은 ROC 곡선 밑의 면적을 구한 것으로, 1에 가까울수록 좋은 수치\n",
        "train_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(label_cols))] # metrixs.AUC() :  리만 합계를 통해 근사 AUC (곡선 아래 영역)를 계산\n",
        "validation_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(label_cols))]\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(model, token_ids, masks, labels):\n",
        "    labels = tf.dtypes.cast(labels, tf.float32)\n",
        "\n",
        "    # with ... as 구문을 사용하게 되면 파일을 열고(open) 해당 구문이 끝나면 파일이 자동으로 닫히게 됨(close). \n",
        "    # with open(파일 경로, 모드) as 파일 객체:\n",
        "    # GradientTape : (자동 미분을 위해 기록)  테이프는 역방향 패스(오차 역전파)의 그래디언트를 계산하기 위해 정방향 패스에 기록할 연산을 알아야 합니다.\n",
        "    # TensorFlow 는 중간 연산 과정(함수, 연산)을 테이프(tape)에 차곡차곡 기록해주는 Gradient tapes 를 제공\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(token_ids, attention_mask=masks)\n",
        "        loss = loss_object(labels, predictions)\n",
        "    # loss : 예측 값과 실제 값의 차이를 수치화함( 손실함수 )\n",
        "        \n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables), 1.0)\n",
        "    # model.trainable_variables : 현재 모델에서의 가중치 중에서 학습해야 할 가중치는 trainable_variables에 있다.\n",
        "    # tape.gradient : 모델의 훈련 가능한 변수에 대한 그래디언트를 계산\n",
        "    # apply_gradients : 변수에 그라디언트를 적용\n",
        "\n",
        "\n",
        "    # loss 평균 값 구함.\n",
        "    train_loss(loss)\n",
        "\n",
        "    # train_auc_metrics : AUC (곡선 아래 영역)를 계산\n",
        "    for i, auc in enumerate(train_auc_metrics):\n",
        "        auc.update_state(labels[:,i], predictions[:,i])\n",
        "\n",
        "    \n",
        "@tf.function\n",
        "def validation_step(model, token_ids, masks, labels):\n",
        "    labels = tf.dtypes.cast(labels, tf.float32)\n",
        "\n",
        "    predictions = model(token_ids, attention_mask=masks, training=False)\n",
        "    v_loss = loss_object(labels, predictions)\n",
        "\n",
        "    validation_loss(v_loss)\n",
        "    for i, auc in enumerate(validation_auc_metrics):\n",
        "        auc.update_state(labels[:,i], predictions[:,i])\n",
        "\n",
        "def train(model, train_dataset, val_dataset, train_steps_per_epoch, val_steps_per_epoch, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        print('=' * 50, f\"EPOCH{epoch}\",'='*50)\n",
        "        start = time.time()\n",
        "\n",
        "\n",
        "        for i, (token_ids, masks, labels) in enumerate(tqdm(train_dataset, total=train_steps_per_epoch)):\n",
        "            train_step(model, token_ids, masks, labels)\n",
        "            if i % 1000 == 0:\n",
        "                print(f'\\nTrain Step: {i}, Loss: {train_loss.result()}')\n",
        "                for i, label_name in enumerate(label_cols):\n",
        "                    print(f\"{label_name} roc_auc {train_auc_metrics[i].result()}\")\n",
        "                    train_auc_metrics[i].reset_states()\n",
        "        \n",
        "        for i, (token_ids, masks, labels) in enumerate(tqdm(val_dataset, total=val_steps_per_epoch)):\n",
        "            validation_step(model, token_ids, masks, labels)\n",
        "        \n",
        "        print(f'\\nEpoch {epoch+1}, Validation Loss: {validation_loss.result()}, Time: {time.time()-start}\\n')\n",
        "\n",
        "        for i, label_name in enumerate(label_cols):\n",
        "            print(f\"{label_name} roc_auc {validation_auc_metrics[i].result()}\")\n",
        "            validation_auc_metrics[i].reset_states()\n",
        "\n",
        "        print('\\n')\n",
        "\n",
        "train(model, train_dataset, validation_dataset, train_steps_per_epoch=steps_per_epoch, val_steps_per_epoch=validation_steps, epochs=NR_EPOCHS)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gPDE3KbzLNZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4SZZOlRLs5Pv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}